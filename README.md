# titanic-ml-lab

## Overview:
This lab helps us understand a complete end-to-end machine learning workflow from local development to cloud developement. 
The project involves:
- building a titanic survival prediction model using scikit-learn, implementing proper version control with Git and Github
- implementing proper version control with Git and GitHub
- creating an api using FastAPI
- deploying the model on a virtual machine on Microsoft Azure

### PART 1 - Local developement Setup
1. I created a new github repo named titanic-ml-lab. The repo is public (i will change it to private later)
2. Then there were a few steps related to SSH keys configuration. We used SSH keys since they are more secure than passwords, prevents us from having to input credentials repeatedly, and is also the industry standard for authentication. At first it was confusing trying to understand having multiple SSH keys like one for github, one for Azure. But then later i found out that each connection needs its own key pair.
3. I learned some very important git commands like config, status, add, push and so on. git add is used to stage files for commit. The staging area allows selective commits which keeps the version history clean.
4. Kaggle API setup involved creating an account on kaggle, downloading kaggle.json, creating a .kaggle directory iin the home folder, and how to set proper file permissions for security (chmod 600)
5. Installed Kaggle CLI to download the titanic dataset.

### PART 2: Machine learning Model Development
Note: I misunderstood some things in the lab document and ended up blundering the project structure.  Because of this one mistake, all the steps further affected my work negatively. The structure showed titanic_code_only although we had been using titanic-ml-lab in all the previous steps. I ended up creating a nested folder structure but then realized all the files should be directly in the root. 
1. We were provided with the environment file already. This file ensures reproducibility across different machines, avoids dependency conflicts and makes it easy for other to replicate our setup.
2. Data preprocessing pipeline using scikit learn which ensures preprocessing steps are applied consistently to training and test data.
3. For model training, we chose the Random Forest Classifier since it handles both numerical and categorical features well. it is also less prone to overfitting than single decision tress. We also used GridSearchCV for hyperparameter tuning and saved the best model using joblib.
4. For model testing, running "python main.py --mode test" loads the saved model pipeline, reads test.csc data, applies the same preprocessing automatically via pipeline, generates predictions, and saves them to outputs/predictions.csv.
5. git version control for model (creating .gitignore, tags). Tags mark specific commits that represent releases which makes it easy to track which code produced which model.

### PART 3: Mock deployment with FastAPI
1. I created a separate branch to keep training code separate from deployment code. Industry best practice involves keeping production environment clean from training data or experimental code. So the training and testing scripts were removed along with any training data not needed for inference.
2. Dependencies had to be updated for production and the main.py defines API endpoins, loads trained model at startup. It has an endpoint /predict for predictions. The prediction module (predict.py) defines the input schema using Pydantic, validates incoming data and returns predictions.
3. For testing the API, it was run locally. uvicorn main:app --reload to start the FastAPI server and then we go to http://127.0.0.1:8000/docs on our browser to test /predict endpoint. FastAPI automatically generates interactive documentation making API testing much easier.

### PART 4: Azure cloud deployment
1. I configured a VM on Azure based on the instructions given in the lab document. SSH keys were generated by azure.
2. We setup the VM enviroment by updating package lists and installed essential tools. Then we installed miniconda on the VM. Miniconda provides same environment management as local machine and therefore ensures consistency.
3. Since the Azure VM is a different computer, so it needs its own SSH key for github. The repo was cloned and kaggle was setup on the VM.
4. Then i trained a model again on azure and the model outout was saved to models/titanic_pipeline_cloud.joblib. Everything was commited and tagged. 

<img width="1440" height="900" alt="1" src="https://github.com/user-attachments/assets/9fc4a1c8-24f3-4836-b1f1-813762c59df1" />
<img width="1388" height="829" alt="2" src="https://github.com/user-attachments/assets/3c7f6f37-5c33-427c-b2a8-a6b6c49c852e" />
<img width="1440" height="283" alt="3" src="https://github.com/user-attachments/assets/8b4c6ced-63d2-4a26-90b7-5313e7a306dc" />
<img width="1440" height="900" alt="5" src="https://github.com/user-attachments/assets/57b3b445-1408-444f-b0c5-4b25f5cf1f28" />
<img width="1440" height="900" alt="6" src="https://github.com/user-attachments/assets/9b5aed5a-8dfa-4beb-ab53-23ec25603e9d" />
<img width="1440" height="900" alt="7" src="https://github.com/user-attachments/assets/16ed9b9c-3d57-4ffb-a394-93745dea81fd" />



## Git commands
- git config --global user.name "Your Name" : sets your name that will appear on all your commits. Used during one-time setup on a new machine
- git config --global user.email "your@email.com" : sets your email that will appear on all commits. Also used during the one-time setup on a new machine
- git config --list : shows all git configuration settings. Used to verify your name, email and other settings are correct. it displays settings like user.name, user.email, etc
- git clone <repo-url> : downloads a complete copy of a repo from github to your computer
- git remote -v : shows which remote repo your local folder is connected to. you can use this to verify you're pushing from the correct github repo
- git status : shows which files have changed, which are staged, and which are untracked. used before commiting to see what you are about to save. red mean unmodified/untracked, green means staged and ready to commit
- git log --oneline: shows a simplified list of all past commits. outputs a comit id and commit message
- git branch : lists all branches and highlights which one you are currently on
- git branch -a : lists all branches (local and remote)
- git add filename: stages a specific file for the next commit. Used when you want to commit only certain files
- git add . : stages all changed files in the current directory for commit
- git reset : unstages all staged filed (not deletes) when you accidentally staged files you didn't want to commit
- git commit -m "Message" : saves staged changes with a descriptive message. Used after staging files, to create a permanent snapshot
- git push origin main : uploads your commits from local main branch to Github
- git push origin <tag-name> : uploads a specific tag to github / pushes one tag instead of all tags
- git push origin --tags: uploads all your tags to github
- git pull origin main : downloads commits from github and merges them into your local branch
- git checkout -b <branch-name> : creates a new branch and switches to it immediately
- git switch <branch-man> : switches to an existing branch
- git tag -a v1.0 -m "Message" : creates a labeled checkpoint in your project history
- git tag: lists all tags in your repository
- git rm <file> : deletes a file and stages the deletion for commit
- git rm -r <folder> : recursively deletes a folder and all its contents


